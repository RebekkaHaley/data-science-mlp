## Welcome

This repo contains code for a multilayer perceptron (MLP), also known as a feed-forward neural network. It has been implemented in both simple 2-layer and n-layer versions.

This project began as an exercise during a university module, but has since been fleshed out into two-layer and n-layer versions.

## Multilayer perceptron

MLPs involve two stages: (1) the forward pass, and (2) back-propagation.

MLPs are based on the idea of taking multiple single perceptrons and connecting them in a network. The input is turned into output during the forward pass via matrix multiplication. Subsequently, the weights are updated during back-propagation, where gradient descent and differentiation via the chain rule are utilised.

## Multilayer perceptron

This folder pertains to a machine learning tool called the multilayer perceptron (MLP). MLPs involve two stages: (1) the forward pass, and (2) back-propagation.

MLPs are based on the idea of taking multiple single perceptrons and connecting them in a network. The input is turned into output during the forward pass via matrix multiplication. Subsequently, the weights are updated during back-propagation, where gradient descent and differentiation via the chain rule are utilised.

This project originally began as an exercise during a university module, but has since been fleshed out into a two-layer version and an N-layer version.